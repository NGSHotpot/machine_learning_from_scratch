# K Means

Previously, we introduced linear regression, logistic regression, softmax regression, etc, these methods are all supervised learning methods. That is, for training data, we know what the feature is and what the labels (dependent variable) are for training data. 

Here K means is an unsupervised learning methods, we only know the features of the data, and we want to category the input samples into pre-defined K clusters.

## Introduction

The K in the name K means algorithms means the number of clusters the input data should be grouped into. Similar things always behave the same support the algorithms. Assume we have ![equation](http://latex.codecogs.com/gif.latex?m) points in high-dimension space, each of the points have ![equation](http://latex.codecogs.com/gif.latex?m) dimensions. We represent each point using a row vector with m elements, for example: ![equation](http://latex.codecogs.com/gif.latex?x=[x_1,x_2,...,x_m]). The number of clusters ![equation](http://latex.codecogs.com/gif.latex?K) is a hyper-parameter, which is pre-defined by the user.

### K-means algorithm process

The mission of K-means clustering is categoring the input points into ![equation](http://latex.codecogs.com/gif.latex?K) clusters. Here's how the K-means process:

First, randomly select K points from the input points, and regarding them as the center of the K-clusters.

Then repeat the following process until it is convergence:

1. Group all the input points into the K-clusters based the distance to the K-clusters' center points (Euclidean distance).

2. Update the center for each of the K-clusters using the center of the points belong to each cluster.

After all we get K clusters. Below is a dynamic graph for how K-means works (Will upload soon).

### Python codes from scratch for K-means clutering

Below comes the codes implementation the K-means algorithm:

```python
import os, glob,random
import numpy as np

class KMeans:
  """
  K means clustering methods
  """
  def __init__(self):
    """
    @param self.cluster_res, np.array, a list indicating the cluster results.
    @param self.centers, the centers of the K means, np.array with size K x m
    """
    self.cluster_res = None
    self.centers = None
    
  def train(self, X, K, methods="euclidean", n_iter=100):
    """
    Main algorithm for K means cluster.
    @param X, input data, np.array, example np.array([[1,2,3,..],[2,3,4,...],...]) with n x m size, n samples, each sample have m features.
    @param K, the hyper-parameter for k means clustering.
    @param methods, methods to calculate the distance between points
    @param n_iter, number of iteration to run
    @return, fill the self.cluster_res
    """
    all_methods = set(["euclidean"])
    if methods not in all_methods:
      print "Error, the methods much be one of ", all_methods
      return
    n = len(X)
    m = len(X[0])
    indexes = range(n)
    samples = random.sample(indexes, K)
    self.centers = X[samples]
    if methods == "euclidean":
      distance = np.sqrt(np.sum(np.square(X - self.centers.rekshape(K, 1, m)), axis=2))
      self.cluster_res = distance.argmin(axis=0)
      for i in range(n_iter):
        for j in range(K):
          tmparray = X[self.cluster_res==j]
          self.centers[j] = np.mean(tmparray,axis=0)
        distance = np.sqrt(np.sum(np.square(X - self.centers.rekshape(K, 1, m)), axis=2))
        self.cluster_res = distance.argmin(axis=0)

```

Codes can also be found at the [Kmeans](https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/codes/kmeans.py).

## Summary

Here we introduced the thought behind the K means clustering. K is a hyper parameter which need the users to decide with to set.
