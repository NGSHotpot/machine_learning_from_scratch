# Softmax

Previously, we have introduced [logistic regression](https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/algorithm/3.logisticRegression.md),
Logistic regression can solve the binary classification problem, softmax regression is a generalize of logistic regression that we could solve multi-category
classification problem using softmax.

Here is how it comes:

## Loss function of softmax regression

We firstly re-call the loss function for logistic regression:

![equation](http://latex.codecogs.com/gif.latex?J(\theta)=-\sum_{i=1}^{n}(y_ilog(a_i)+(1-y_i)log(1-a_i)))

where

![equation](http://latex.codecogs.com/gif.latex?a_i=\frac{1}{1+e^{-x_i\theta}})

In logistic regression, we will categories the samples into to 0-class and 1-class. The ![equation](http://latex.codecogs.com/gif.latex?a_i) is always regarded as the probability to category the sample to 1-class, then the probability for the 0-class is ![equation](http://latex.codecogs.com/gif.latex?1-a_i). 

Instead, in softmax regression, we would like to category the samples into ![equation](http://latex.codecogs.com/gif.latex?K) classes. The softmax function have much more weight parameters and bias parameters for softmax regression: we have ![equation](http://latex.codecogs.com/gif.latex?m\times{K}) weight variables, ![equation](http://latex.codecogs.com/gif.latex?K) bias parameters and also we have ![equation](http://latex.codecogs.com/gif.latex?K) dependent variables. The dependent variable for each sample can be expressed as ![equation](http://latex.codecogs.com/gif.latex?y=[y_1,y_2,...,y_K]), in which of one of the K numbers is 1, others are all 0.

We first get the representation of softmax function for each sample, for simplification, I will skip the subscript ![equation](http://latex.codecogs.com/gif.latex?i) for each sample:

![equation](http://latex.codecogs.com/gif.latex?a_{k}=\sum_{j=1}^{m}x_{j}w_{jk}+b_k=\sum_{j=1}^{m+1}x_{j}\theta_{jk},k=1,2,...,K)

Then we normalize it:

![equation](http://latex.codecogs.com/gif.latex?s_{j}=\frac{e^{a_{j}}}{\sum_{k=1}^{K}e^{a_{k}}})

It is easy to get that:

![equation](http://latex.codecogs.com/gif.latex?\sum_{k=1}^{K}s_{k}=1)

Then the loss function of the softmax regression is defined as:

![equation](http://latex.codecogs.com/gif.latex?J(\theta;x)=-\sum_{k=1}^{K}y_{k}log(s_{k}))

Then the loss function across all samples will be:

![equation](http://latex.codecogs.com/gif.latex?J(\theta)=-\sum\sum_{k=1}^{K}y_{k}log(s_{k}))


## Gradient of softmax loss function

Also we start with the derivatives respect to each parameter for each samples. For one specific sample ![equation](http://latex.codecogs.com/gif.latex?x), we assume true class of it is the i-class. Then the cost function can be re-write as:

![equation](http://latex.codecogs.com/gif.latex?J(\theta;x)=-\sum_{k=1}^{K}y_{k}log(s_{k})=-y_{i}log(s_{i}))

Then we could generate the derivatives:

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{J(\theta;x)}}{\partial{s_i}}=-\frac{y_i}{s_i})

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{s_i}}{\partial{a_i}}=s_i(1-s_i))

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{s_i}}{\partial{a_j}}=-s_is_j)

here ![equation](http://latex.codecogs.com/gif.latex?i\ne{j}), and

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{a_j}}{\partial{\theta_{kj}}}=x_k)

With the chain rule in summary, we generate the derivatives for each parameter for one input sample,

For ![equation](http://latex.codecogs.com/gif.latex?i=j), then

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{J(\theta,x)}}{\partial{\theta_{kj}}}=\frac{\partial{J(\theta;x)}}{\partial{s_i}}\frac{\partial{s_i}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{\theta_{kj}}}=y_i(s_i-1)x_k=(s_i-1)x_k)

and similarly for ![equation](http://latex.codecogs.com/gif.latex?i\ne{j}),

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{J(\theta,x)}}{\partial{\theta_{kj}}}=\frac{\partial{J(\theta;x)}}{\partial{s_i}}\frac{\partial{s_i}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{\theta_{kj}}}=y_is_jx_k=s_jx_k)

To implement more easily and more fast, we also vectorize above derivatives, then we get:

![equation](http://latex.codecogs.com/gif.latex?\frac{\nabla{J(\theta,x)}}{\nabla{\theta}}=(s-y)x^T)

With gradient above, it is quite easy to generate gradient across all the samples. Check the python implementation codes below.

## One more problem in python implementation

We previously found that, we should calculate the softmax function for input sample, which may exceed the max limit of float number computing using computer. For example, when some ![equation](http://latex.codecogs.com/gif.latex?a_i) is very big, 9999 for examples, then when computing the softmax function, we need to compute ![equation](http://latex.codecogs.com/gif.latex?e^{a_i}=e^9999), this will definitely exceed the max float number limit. So, computers can not solve this problem, then how can we implementation? (Note, does this happens in Logistic Regression? Yes! Will also put a modified version to logistic regression)

We use following formulas to calculate the softmax function instead:

![equation](http://latex.codecogs.com/gif.latex?s_i=\frac{e^{a_i}}{\sum_{k=1}^{K}e^{a_k}}=\frac{e^{a_i-M}}{\sum_{k=1}^{K}e^{a_k-M}})

When set 

![equation](http://latex.codecogs.com/gif.latex?M=max([a_1,a_2,...,a_K]))

Then we will never reach the positive infinity. 

## Python implementation of softmax regression using gradient descent

With gradient / derivatives calculated above, we could easily implement the softmax regression with python.

```python
import numpy as np
```


## Summary

We introduced how softmax regression works and the python codes implementing the softmax regression with gradient descent. With knowledges introduced, we can now construct basic neural networks. I am updating the "deep learning from scratch" at the same time after this softmax. You can check the link if you would like to know how deep neural networks works: [Deep Learning from Scratch](https://github.com/chenxingwei/deep_learning_from_scratch)
