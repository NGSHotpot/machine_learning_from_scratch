# Softmax

Previously, we have introduced [logistic regression](https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/algorithm/3.logisticRegression.md),
Logistic regression can solve the binary classification problem, softmax regression is a generalize of logistic regression that we could solve multi-category
classification problem using softmax.

Here is how it comes:

## Loss function of softmax regression

We firstly re-call the loss function for logistic regression:

![equation](http://latex.codecogs.com/gif.latex?J(\theta)=-\sum_{i=1}^{n}(y_ilog(a_i)+(1-y_i)log(1-a_i)))

where

![equation](http://latex.codecogs.com/gif.latex?a_i=\frac{1}{1+e^{-x_i\theta}})

In logistic regression, we will categories the samples into to 0-class and 1-class. The ![equation](http://latex.codecogs.com/gif.latex?a_i) is always regarded as the probability to category the sample to 1-class, then the probability for the 0-class is ![equation](http://latex.codecogs.com/gif.latex?1-a_i). 

Instead, in softmax regression, we would like to category the samples into ![equation](http://latex.codecogs.com/gif.latex?K) classes. The softmax function have much more weight parameters and bias parameters for softmax regression: we have ![equation](http://latex.codecogs.com/gif.latex?m\times{K}) weight variables, ![equation](http://latex.codecogs.com/gif.latex?K) bias parameters and also we have ![equation](http://latex.codecogs.com/gif.latex?K) dependent variables. The dependent variable for each sample can be expressed as ![equation](http://latex.codecogs.com/gif.latex?y=[y_1,y_2,...,y_K]^T), in which of one of the K numbers is 1, others are all 0.

We first get the representation of softmax function for each sample, for simplification, I will skip the subscript ![equation](http://latex.codecogs.com/gif.latex?i) for each sample:

![equation](http://latex.codecogs.com/gif.latex?a_{k}=\sum_{j=1}^{m}x_{j}w_{jk}+b_k=\sum_{j=1}^{m+1}x_{j}\theta_{jk},k=1,2,...,K)

Then we normalize it:

![equation](http://latex.codecogs.com/gif.latex?s_{j}=\frac{e^{a_{j}}}{\sum_{k=1}^{K}e^{a_{k}}})

It is easy to get that:

![equation](http://latex.codecogs.com/gif.latex?\sum_{k=1}^{K}s_{k}=1)

Then the loss function of the softmax regression is defined as:

![equation](http://latex.codecogs.com/gif.latex?J(\theta;x)=-\sum_{k=1}^{K}y_{k}log(s_{k}))

Then the loss function across all samples will be:

![equation](http://latex.codecogs.com/gif.latex?J(\theta)=-\sum\sum_{k=1}^{K}y_{k}log(s_{k}))


## Gradient of softmax loss function

Also we start with the derivatives respect to each parameter for each samples. For one specific sample ![equation](http://latex.codecogs.com/gif.latex?x), we assume true class of it is the i-class. Then the cost function can be re-write as:

![equation](http://latex.codecogs.com/gif.latex?J(\theta;x)=-\sum_{k=1}^{K}y_{k}log(s_{k})=-y_{i}log(s_{i}))

Then we could generate the derivatives:

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{J(\theta;x)}}{\partial{s_i}}=-\frac{y_i}{s_i})

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{s_i}}{\partial{a_i}}=s_i(1-s_i))

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{s_i}}{\partial{a_j}}=-s_is_j)

here ![equation](http://latex.codecogs.com/gif.latex?i\ne{j}), and

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{a_j}}{\partial{\theta_{kj}}}=x_k)

With the chain rule in summary, we generate the derivatives for each parameter for one input sample,

For ![equation](http://latex.codecogs.com/gif.latex?i=j), then

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{J(\theta,x)}}{\partial{\theta_{kj}}}=\frac{\partial{J(\theta;x)}}{\partial{s_i}}\frac{\partial{s_i}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{\theta_{kj}}}=y_i(s_i-1)x_k=(s_i-1)x_k)

and similarly for ![equation](http://latex.codecogs.com/gif.latex?i\ne{j}),

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{J(\theta,x)}}{\partial{\theta_{kj}}}=\frac{\partial{J(\theta;x)}}{\partial{s_i}}\frac{\partial{s_i}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{\theta_{kj}}}=y_is_jx_k=s_jx_k)

To implement more easily and more fast, we also vectorize above derivatives, then we get:

![equation](http://latex.codecogs.com/gif.latex?\frac{\nabla{J(\theta,x)}}{\nabla{\theta}}=(s-y)x^T)


## Python implementation of softmax regression using gradient descent

With gradient / derivatives calculated above, we could easily implement the softmax regression with python.

```python
import numpy as np
```


## Summary


