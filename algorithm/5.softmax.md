# Softmax

Previously, we have introduced [logistic regression](https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/algorithm/3.logisticRegression.md),
Logistic regression can solve the binary classification problem, softmax regression is a generalize of logistic regression that we could solve multi-category
classification problem using softmax.

Here is how it comes:

## Loss function of softmax regression

We firstly re-call the loss function for logistic regression:

![equation](http://latex.codecogs.com/gif.latex?J(\theta)=-\sum_{i=1}^{n}(y_ilog(a_i)+(1-y_i)log(1-a_i)))

where

![equation](http://latex.codecogs.com/gif.latex?a_i=\frac{1}{1+e^{-x_i\theta}})

In logistic regression, we will categories the samples into to 0-class and 1-class. The ![equation](http://latex.codecogs.com/gif.latex?a_i) is always regarded as the probability to category the sample to 1-class, then the probability for the 0-class is ![equation](http://latex.codecogs.com/gif.latex?1-a_i). 

Instead, in softmax regression, we would like to category the samples into ![equation](http://latex.codecogs.com/gif.latex?K) classes.
