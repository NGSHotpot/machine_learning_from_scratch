# Softmax

Previously, we have introduced [logistic regression](https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/algorithm/3.logisticRegression.md),
Logistic regression can solve the binary classification problem, softmax regression is a generalize of logistic regression that we could solve multi-category
classification problem using softmax.

Here is how it comes:

## Loss function of softmax regression

We firstly re-call the loss function for logistic regression:

![equation](http://latex.codecogs.com/gif.latex?J(\theta)=-\sum_{i=1}^{n}(y_ilog(a_i)+(1-y_i)log(1-a_i)))

where

![equation](http://latex.codecogs.com/gif.latex?a_i=\frac{1}{1+e^{-x_i\theta}})

In logistic regression, we will categories the samples into to 0-class and 1-class. The ![equation](http://latex.codecogs.com/gif.latex?a_i) is always regarded as the probability to category the sample to 1-class, then the probability for the 0-class is ![equation](http://latex.codecogs.com/gif.latex?1-a_i). 

Instead, in softmax regression, we would like to category the samples into ![equation](http://latex.codecogs.com/gif.latex?K) classes. The softmax function have much more weight parameters and bias parameters for softmax regression: we have ![equation](http://latex.codecogs.com/gif.latex?m\times{K}) weight variables, ![equation](http://latex.codecogs.com/gif.latex?K) bias parameters and also we have ![equation](http://latex.codecogs.com/gif.latex?K) dependent variables.

We first get the representation of softmax function for each sample:

![equation](http://latex.codecogs.com/gif.latex?a_{ik}=\sum_{j=1}^{m}x_{ij}w_{jk}+b_k=\sum_{j=1}^{m+1}x_{ij}\theta_{jk},k=1,2,...,K)

Then we normalize it:

![equation](http://latex.codecogs.com/gif.latex?s_{ij}=\frac{e^{a_{ij}}}{\sum_{k=1}^{K}e^{ik}})

It is easy to get that:

![equation](http://latex.codecogs.com/gif.latex?\sum_{k=1}^{K}s_{ik}=1)

Then the loss function of the softmax regression is defined as:

![equation](http://latex.codecogs.com/gif.latex?J(\theta;x_i)=-\sum_{k=1}^{K}y_{k}log(s_{ik}))

Then the loss function across all samples will be:

![equation](http://latex.codecogs.com/gif.latex?J(\theta)=-\sum_{i=1}^{n}\sum_{k=1}^{K}y_{ik}log(s_{ik}))


## Gradient of softmax loss function

Also we start with the derivatives respect to each parameter for each samples:

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{J(\theta;x_i)}}{\partial{\theta_j}})




## Summary


