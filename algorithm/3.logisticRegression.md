# Logistic Regression

We have previously go over linear regression with mathematical methods and gradient regression. Here we will implement logistic regression
using gradient descent.

In linear regression, we have independent variables and dependent variable, and the dependent variable is continuous variable. Everything is the same for logistic regression but the dependent variable is not a continuous variable. The dependent variable in logistic regression is a binary variable, for example, whether is a female or male, something like this. So the dependent variable is always expressed as:

![equation](http://latex.codecogs.com/gif.latex?y=[0,1,1,0,...,1,0,1])

We used to set the binary variable to 0 and 1. Of course we can solve this problem with linear regression directly, as input and output of logistic regression also satisfy the requirement for linear regression. But the results are always bad. 

## Sigmoid function

We first give the function of sigmoid function and curves as shown blow:

![equation](http://latex.codecogs.com/gif.latex?h(x)=\frac{1}{1+e^{-x}})

and its curve:

![](https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/images/lr3_001.png)

The sigmoid function have defination for every real number, and its range of the dependent variable is from 0 to 1. Taking the category 1 as standard, sigmoid function can be thought to be the probability the input sample is belong to the category 1. Notably, when sigmoid function returns value larger than 0.5, meaning the sample has a probability larger than 0.5 to be in categories 1, so we can category it into the 1 class. Otherwise, with sigmoid function output less than 0.5, category it into the 0 class.

Advantages to use logistic regression:

1. Sigmoid function is derivable and it is very easy to get its derivatives.
2. Probability explaination of sigmoid function gives understanding of the function.

## Cross entropy loss function

We can't use ordinary least square (OLS) loss function and the mean absolute error (MAE) loss function. It we use these two loss functions, it goes back to linear regression.

The loss function we use the called the cross entropy loss function, here will include some simple knowledge from information theory. I will very briefly introduce the basis of the cross entropy:

1. Firstly we need to know  what is information (in information theory). 

Many things happen randomly, but the things happen with different information. If one thing here happens with probability ![equation](http://latex.codecogs.com/gif.latex?p), then the information of it can be defined as ![equation](http://latex.codecogs.com/gif.latex?-log(p)). 

That is things happen with low probability takes more information. This can be explained or understand through a real world example: We define two things, A is a driver overspeeds and B is car accident caused 10 people die and 20 people injury. Human can easy find B is much more serious than A. A journalist will also report B instead of A because B takes more information. In dayly life, overspeed happens over again and again, but car accident with so many people die and injury rarely happens, the probability for overspeed is much higher than the car accident. Based on the defination of information, B takes more information than A. That is why B will be reported by journalist and B will be known to much more people.

2. Secondly we need to know what is entropy.

Entropy can be understand as the average information of a distribution. For example, we toss the coin, having head and tail with probability 0.5 each. Then the entropy for toss a coin is calculated as 

![equation](http://latex.codecogs.com/gif.latex?entropy=-0.5log(0.5)-0.5log(0.5))

Two two terms in the formula represent toss a head and a tail respectively, more generally, the entropy is defined as:

![equation](http://latex.codecogs.com/gif.latex?entropy=E[-log(p)]=-\sum_{i=1}^{n}p_ilog{p_i})

After then, we comes to the cross entropy, we have real dependent variable (0 or 1), which is also called labels: ![equation](http://latex.codecogs.com/gif.latex?y=[y_1,y_2,...,y_n]). And at the same time, we have the model predicted labels: ![equation](http://latex.codecogs.com/gif.latex?a=[a_1,a_2,...,a_n]), which range from 0 to 1, indicating the probability to be categoried into the 1 class. 

Take out one of them for example, that we have real label ![equation](http://latex.codecogs.com/gif.latex?y_i) and predicted probability ![equation](http://latex.codecogs.com/gif.latex?a_i), this means we have ![equation](http://latex.codecogs.com/gif.latex?a_i) probability to category the sample into 1 class, and we also have ![equation](http://latex.codecogs.com/gif.latex?1-a_i) probability to category the sample into 0 class. Similar as the entropy, the cross entropy multiple the real label as shown below:

![equation](http://latex.codecogs.com/gif.latex?J_i=-y_ilog(a_i)-(1-y_i)log(1-a_i))

Finally, the cross entropy is defined as:

![equation](http://latex.codecogs.com/gif.latex?J=-\sum_{i=1}^{n}(y_ilog(a_i)+(1-y_i)log(1-a_i)))

## Logistic regression

Then is the detail process of logistic regression, similarly as linear regression, we have independent variables and dependent variables. 
