# Regularization

## Introduction

In previous linear regression and logistic regression examples, we have shown how to learn the weight and bias, in both mathematics way and gradient descent way. In previous examples, we must have training set for the algorithm to learn the parameters, but in real worlds, we not only have training data, but also have testing data, which maybe much much more than  the training data.

So we should ask a question, do the parameters learned from the training data also suitable for the testing data? Not always! When parameters can exapain training data very well, but can poorly explain the testing data, we can say the parameters have beening overfit to the training data. We do not like overfit.

Regularization is one of the strategies to deal with overfit, which could help to improve the generative of the model and at the same time maybe hurt the performance in training data.

## Popular regularization strategies

The easiest and most popular regularization strategies are L1-norm and L2-norm regularization. At the same time, Elastic net which is L1-norm plus L2-norm and Max norm regularization are also very popular regularization algorithms.

### L1-norm regularization

### L2-norm regularization

### Elastic net regularization

### Max nomr regularization


## Regularization in regression

### L1-norm regularization in linear regression

### L1-norm regularization in logistic regression


