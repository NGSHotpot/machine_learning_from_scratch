# Regularization

## Introduction

In previous linear regression and logistic regression examples, we have shown how to learn the weight and bias, in both mathematics way and gradient descent way. In previous examples, we must have training set for the algorithm to learn the parameters, but in real worlds, we not only have training data, but also have testing data, which maybe much much more than  the training data.

So we should ask a question, do the parameters learned from the training data also suitable for the testing data? Not always! When parameters can exapain training data very well, but can poorly explain the testing data, we can say the parameters have beening overfit to the training data. We do not like overfit.

Regularization is one of the strategies to deal with overfit, which could help to improve the generative of the model and at the same time maybe hurt the performance in training data.

## Popular regularization strategies

The easiest and most popular regularization strategies are L1-norm and L2-norm regularization. At the same time, Elastic net which is L1-norm plus L2-norm are also very popular regularization algorithms.

In general, we add a regularization term to the loss function like:

![equation](http://latex.codecogs.com/gif.latex?J_{new}=J_{\theta}+CR(\theta))

Here ![equation](http://latex.codecogs.com/gif.latex?R(\theta)) is a function of the parameters, and it is also a exact positive increasing function to the absolute value of parameters. Regularization works because, to minimize the loss function, we need to also need to make all the parameters small. A larger weight parameter effect the model much more than a smaller one. This means if some large parameter is overfit to training data, the model is highly probability to be overfit, however, when using smaller parameters, the smaller parameter overfit to training data do not effect the model so much. That's why regularization works.

### L1-norm regularization

The loss function for L1-norm regularization is:

![equation](http://latex.codecogs.com/gif.latex?J_{new}=J_{\theta}+C|\theta|)

here the regularization function is:

![equation](http://latex.codecogs.com/gif.latex?R(\theta)=|\theta|)

### L2-norm regularization

The loss function for L2-norm regularization is:

![equation](http://latex.codecogs.com/gif.latex?J_{new}=J_{\theta}+\frac{1}{2}C{\theta}^2)

here the regularization function is:

![equation](http://latex.codecogs.com/gif.latex?R(\theta)=\frac{1}{2}{\theta}^2)

### Elastic net regularization

The loss function for Elastic net regularization is:

![equation](http://latex.codecogs.com/gif.latex?J_{new}=J_{\theta}+Cp|\theta|+\frac{1}{2}C(1-p){\theta}^2)

here the regularization function is:

![equation](http://latex.codecogs.com/gif.latex?R(\theta)=p|\theta|+\frac{1}{2}(1-p){\theta}^2)


## Regularization in regression

It is quite easy to generate the gradient of the regularization terms for regression problems, in both linear regression and logistic regression. Below I will go over the codes, gradients for L1-norm, L2-norm and Elastic net regularization for both linear regression and logistic regression.

### L2-norm regularization in linear regression

As previous shown, the loss function of L2-norm regularization in linear regression can be expressed as:

![equation](http://latex.codecogs.com/gif.latex?J=\frac{1}{2}\sum_{i=1}^{n}(x_i\theta-y_i)^2+\frac{1}{2}C\sum_{i=1}^{m}\theta^2)


Then let's go over how we can see solve the linear regression with L2-norm regularization.

We have got the gradient for linear regression, here we only calculate the gradient for the regularization term:

![equation](http://latex.codecogs.com/gif.latex?R(\theta)=\frac{1}{2}C\sum_{i=1}^{m}\theta^2)

Similarly, we generate the derivatives for each ![equation](http://latex.codecogs.com/gif.latex?\theta_j):

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{R(\theta)}}{\partial{\theta_j}}=C\theta_j)

Then vectorize the derivatives across all the parameters to generate the gradient:

![equation](http://latex.codecogs.com/gif.latex?\nabla_{\theta}R(\theta)=C\theta)

Combined with previous gradient on the linear regression OLS loss function, we generate the gradient:

![equation](http://latex.codecogs.com/gif.latex?\nabla_{\theta}J(\theta)=\sum_{i=1}^{n}(x_i\theta-y_i)x_{i}+C\theta)

We also implement using numpy:

```python
import os, glob, math
import numpy as np
import random

class linear_regression_GDL2:
  """
  linear regression with OLS loss function plus L2-norm regularization, and solving by gradient descent methods.
  """
  def __init__(self):
    self.rmse = None
    pass
  
  def train(self, X, y, learning_rate=0.001, epoch=30000, tol=1e-4, methods="OLS", C=0.1):
    """
    If data not the required format, this function not works
    @param X, independent variable, example: X = np.array([[1,2,],[1,3,],[1,4], ...])
    @param y, dependent variable, example y = np.array([[1],[2],[3],[4],...])
    Both X and y should be an array.
    @param learning_rate, step size for gradient descent
    @param epoch, times the training process go over the whole datasets
    @param tol, the stop critera, when loss function did not optimize more than tol for 10 times, stop training
    @param methods, the loss function, OLS stands for ordinary least square loss function
    @return, this function will return weight vector w and bias parameter b
    """
    n = len(X)
    self.n = n
    m = len(X[0])
    if len(y) != n:
      print "Error, the input X, y should be the same length, while you have len(X)=%d and len(y)=%d"%(n, len(y))
    
    # following codes reformat the input
    X = np.array(X)
    y = np.array(y)
    
    if len(y.shape) != 1:
      y = y.reshape(n)
    
    # add a 1 column for the bias variable
    X = np.column_stack((X, np.ones(n)))
    self.X = X
    self.y = y
    
    # get the parameter w and b, first initize
    theta = np.random.randn(m+1)
    num = 0
    loss1 = self.getLoss(theta)
    for i in xrange(epoch):
      theta -= learning_rate * self.getGradient(theta, C)
      loss2 = self.getLoss(theta, methods)
      if abs(loss2-loss1) < tol:
        num += 1
      else:
        num = 0
      loss1 = loss2
      self.epoch = i
      #if num >= 10:
      #  break
    self.w = theta[:-1]
    self.b = theta[-1]
    return theta
    
  def getLoss(self, theta, methods="OLS"):
    """
    @param theta, the parameters for the linear model
    @return the loss of this model with given parameters
    """
    if methods == "OLS":
      # divide self.n first to avoid float overflow to some extent.
      return np.sum(np.square((self.X.dot(theta) - self.y)/self.n) * self.n)
    else:
      print 'Error, methods now can only be "OLS"'
  
  def getGradient(self, theta, C):
    """
    Get the gradient of the loss function using parameter theta
    """
    delta = np.sum(self.X.T * (self.X.dot(theta)-self.y), axis=1) + C * theta
    return delta
  
  def predict(self, X, y=None):
    """
    @param X, independent variable.
    Example: X = np.array([[1,2,],[1,3,],[1,4], ...])
    @param y, dependent variable, example y = np.array([[1],[2],[3],[4],...])
    @return, return the predicted dependent variables for input X, or if y is specified, also calculate the RMSE (root mean square error)
    """
    n = len(X)
    X = np.array(X)
    y_pred = X.dot(self.w) + self.b
    self.y_pred = y_pred
    if y != None:
      if n != len(y):
        print "Error, the input X, y should be the same length, while you have len(X)=%d and len(y)=%d"%(n, len(y))
      y = np.array(y)
      y_pred = y_pred.reshape(n)
      if len(y.shape) != 1:
        y = y.reshape(n)
      self.rmse = np.sqrt(np.sum(np.square(y-y_pred))) / n
    return self.y_pred


```

Codes can also be found at [Linear regression with L2-norm regularization](https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/codes/linear_regression_l2.py)

### L2-norm regularization in logistic regression

As previous shown, the loss function of L2-norm regularization in logistic regression can be expressed as:

![equation](http://latex.codecogs.com/gif.latex?J=-\sum_{i=1}^{n}(y_ilog(a_i)+(1-y_i)log(1-a_i))+\frac{1}{2}C\sum_{i=1}^{m}\theta^2)

Then let's go over how we can see solve the logistic regression with L2-norm regularization.

We have got the gradient for linear regression, here we only calculate the gradient for the regularization term:

![equation](http://latex.codecogs.com/gif.latex?R(\theta)=\frac{1}{2}C\sum_{i=1}^{m}\theta^2)

Similarly, we generate the derivatives for each ![equation](http://latex.codecogs.com/gif.latex?\theta_j):

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{R(\theta)}}{\partial{\theta_j}}=C\theta_j)

Then vectorize the derivatives across all the parameters to generate the gradient:

![equation](http://latex.codecogs.com/gif.latex?\nabla_{\theta}R(\theta)=C\theta)

Combined with previous gradient on the linear regression OLS loss function, we generate the gradient:

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{J(x_i)}}{\partial{\theta}}=x_{i}(a_i-y_i)+C\theta)

Python codes using numpy:

```python
import os, glob, math
import numpy as np
import random

class logistic_regression_GD:
  """
  logistic regression with crossentropy loss function, and solving by gradient descent methods.
  """
  def __init__(self):
    self.w = None
    self.b = None
    self.y_pred = None
  
  def train(self, X, y, learning_rate=0.001, epoch=30000, tol=1e-4, methods="crossentropy", C=0.1):
    """
    If data not the required format, this function not works
    @param X, independent variable, example: X = np.array([[1,2,],[1,3,],[1,4], ...])
    @param y, dependent variable, example y = np.array([[1],[2],[3],[4],...])
    Both X and y should be an array.
    @param learning_rate, step size for gradient descent
    @param epoch, times the training process go over the whole datasets
    @param tol, the stop critera, when loss function did not optimize more than tol for 10 times, stop training
    @param methods, the loss function, OLS stands for ordinary least square loss function
    @return, this function will return weight vector w and bias parameter b
    """
    n = len(X)
    self.n = n
    m = len(X[0])
    if len(y) != n:
      print "Error, the input X, y should be the same length, while you have len(X)=%d and len(y)=%d"%(n, len(y))
    
    # following codes reformat the input
    X = np.array(X)
    y = np.array(y)
    
    if len(y.shape) != 1:
      y = y.reshape(n)
    
    # add a 1 column for the bias variable
    X = np.column_stack((X, np.ones(n)))
    self.X = X
    self.y = y
    
    # get the parameter w and b, first initize
    theta = np.random.randn(m+1)
    num = 0
    for i in xrange(epoch):
      theta -= learning_rate * self.getGradient(theta, methods, C=0.1)
      self.epoch = i
    self.w = theta[:-1]
    self.b = theta[-1]
    return theta
  
  def getGradient(self, theta, methods="crossentropy", C):
    """
    Get the gradient of the loss function using parameter theta
    """
    methods_from = ["crossentropy"]
    if methods == "crossentropy":
      a = 1.0 / (1+np.exp(-1.0*(self.X.dot(theta))))- self.y
      a = a.reshape(self.n,1)
      delta = np.sum(self.X*a, axis=0) + C * theta
    else:
      print "Error, the methods parameter can only be ", methods_from
    return delta
  
  def predict(self, X, y=None):
    """
    @param X, independent variable.
    Example: X = np.array([[1,2,],[1,3,],[1,4], ...])
    @param y, dependent variable, example y = np.array([[1],[2],[3],[4],...])
    @return, return the predicted dependent variables for input X, or if y is specified
    """
    n = len(X)
    X = np.array(X)
    y_pred = X.dot(self.w) + self.b
    self.y_pred = y_pred
    if y != None:
      if n != len(y):
        print "Error, the input X, y should be the same length, while you have len(X)=%d and len(y)=%d"%(n, len(y))
      y = np.array(y)
      y_pred = y_pred.reshape(n)
      if len(y.shape) != 1:
        y = y.reshape(n)
      self.rmse = np.sqrt(np.sum(np.square(y-y_pred))) / n
    return self.y_pred
```

Codes can also be found at [Logistic regression with L2-norm regularization](https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/codes/logistic_regression_l2.py)

### Linear regression with L1-norm regularization

### Logistic regression with L1-norm regularization

### Linear regression with Elastic net regularization

### Logistic regression with Elastic net regularization

## Summary


