# Regularization

## Introduction

In previous linear regression and logistic regression examples, we have shown how to learn the weight and bias, in both mathematics way and gradient descent way. In previous examples, we must have training set for the algorithm to learn the parameters, but in real worlds, we not only have training data, but also have testing data, which maybe much much more than  the training data.

So we should ask a question, do the parameters learned from the training data also suitable for the testing data? Not always! When parameters can exapain training data very well, but can poorly explain the testing data, we can say the parameters have beening overfit to the training data. We do not like overfit.

Regularization is one of the strategies to deal with overfit, which could help to improve the generative of the model and at the same time maybe hurt the performance in training data.

## Popular regularization strategies

The easiest and most popular regularization strategies are L1-norm and L2-norm regularization. At the same time, Elastic net which is L1-norm plus L2-norm are also very popular regularization algorithms.

In general, we add a regularization term to the loss function like:

![equation](http://latex.codecogs.com/gif.latex?J_{new}=J_{\theta}+CR(\theta))

Here ![equation](http://latex.codecogs.com/gif.latex?R(\theta)) is a function of the parameters, and it is also a exact positive increasing function to the absolute value of parameters. Regularization works because, to minimize the loss function, we need to also need to make all the parameters small. A larger weight parameter effect the model much more than a smaller one. This means if some large parameter is overfit to training data, the model is highly probability to be overfit, however, when using smaller parameters, the smaller parameter overfit to training data do not effect the model so much. That's why regularization works.

### L1-norm regularization

The loss function for L1-norm regularization is:

![equation](http://latex.codecogs.com/gif.latex?J_{new}=J_{\theta}+C|\theta|)

here the regularization function is:

![equation](http://latex.codecogs.com/gif.latex?R(\theta)=|\theta|)

### L2-norm regularization

The loss function for L2-norm regularization is:

![equation](http://latex.codecogs.com/gif.latex?J_{new}=J_{\theta}+\frac{1}{2}C{\theta}^2)

here the regularization function is:

![equation](http://latex.codecogs.com/gif.latex?R(\theta)=\frac{1}{2}{\theta}^2)

### Elastic net regularization

The loss function for Elastic net regularization is:

![equation](http://latex.codecogs.com/gif.latex?J_{new}=J_{\theta}+Cp|\theta|+\frac{1}{2}C(1-p){\theta}^2)

here the regularization function is:

![equation](http://latex.codecogs.com/gif.latex?R(\theta)=p|\theta|+\frac{1}{2}(1-p){\theta}^2)


## Regularization in regression

It is quite easy to generate the gradient of the regularization terms for regression problems, in both linear regression and logistic regression. Below I will go over the codes, gradients for L1-norm, L2-norm and Elastic net regularization for both linear regression and logistic regression.

### L2-norm regularization in linear regression

As previous shown, the loss function of L2-norm regularization in linear regression can be expressed as:

![equation](http://latex.codecogs.com/gif.latex?J=\frac{1}{2}\sum_{i=1}^{n}(x_i\theta-y_i)^2+\frac{1}{2}C\sum_{i=1}^{m}\theta^2)


Then let's go over how we can see solve the linear regression with L2-norm regularization.

We have got the gradient for linear regression, here we only calculate the gradient for the regularization term:

![equation](http://latex.codecogs.com/gif.latex?R(\theta)=\frac{1}{2}C\sum_{i=1}^{m}\theta^2)

Similarly, we generate the derivatives for each ![equation](http://latex.codecogs.com/gif.latex?\theta_j):

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{R(\theta)}}{\partial{\theta_j}}=C\theta_j)

Then vectorize the derivatives across all the parameters to generate the gradient:

![equation](http://latex.codecogs.com/gif.latex?\nabla_{\theta}R(\theta)=C\theta)

Combined with previous gradient on the linear regression OLS loss function, we generate the gradient:

![equation](http://latex.codecogs.com/gif.latex?\nabla_{\theta}J(\theta)=\sum_{i=1}^{n}(x_i\theta-y_i)x_{i}+C\theta)

### L2-norm regularization in logistic regression

As previous shown, the loss function of L2-norm regularization in logistic regression can be expressed as:

![equation](http://latex.codecogs.com/gif.latex?J=-\sum_{i=1}^{n}(y_ilog(a_i)+(1-y_i)log(1-a_i))+\frac{1}{2}C\sum_{i=1}^{m}\theta^2)


