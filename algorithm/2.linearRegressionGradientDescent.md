# Linear Regression with Gradient Descent

Here I will introduce the gradient descent algorithm and its application to solve linear regression problems with OLS loss function, then
I will extend it to some other loss functions such as L1-norm loss function.

## Gradient Descent

As previously show in ![linear regression](https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/algorithm/1.linearRegression.md), we can solve linear regression  with derivatives. But many complex functions do not have an analytical solution, or they are hard and complex to solve through a mathematics way. Then comes for Gradient Descent, which can be used to solve minimize or maximize problems.

### How gradient descent works?

We have previously in the linear regression part shown that gradient is the derivatives of a vector or matrix. Here we will go over how gradient descent works in low-dimensional examples. 

If we have a function like ![equation](http://latex.codecogs.com/gif.latex?y=x^2) with its curve as figure below. 

![](https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/images/lr_001.png)

We have two points on the curve, one on the left and the other on the right. We firstly take a look at the point A, with a coordinate ![equation](http://latex.codecogs.com/gif.latex?(-2,4)), with very basic math knowledge we will get the tangent line at A is ![equation](http://latex.codecogs.com/gif.latex?y=-4x-4) as indicated by the red line. And the point B with coordinate ![equation](http://latex.codecogs.com/gif.latex?(1,1)) on the right, similarly we have its tangent line with function ![equation](http://latex.codecogs.com/gif.latex?X=2x-1) as indicated by the blue line.

The derivitives (gradient) at point A keeps the same for the curve and the tangent line, then we have derivative (gradient, maybe also can be called slope) at A and B are ![equation](http://latex.codecogs.com/gif.latex?-4) and ![equation](http://latex.codecogs.com/gif.latex?2), respectively.

The gradient have orientations, in the example above, the orientation of gradient at A is tend to negative infinity and the orientation of gradient at B is tend to posive infinity. How can we get to the minimum value in the above example? It is obviously shown that we should tend positive at point A, in the opposite orientation of the gradient at point A. Similarly, we should tend negative at point B which is also in the opposite orientation of the gradient at point B.

It is similar at high-dimensional parameter spaces.

### Gradient descent for OLS linear regression

Updating paramters at the negative orientation of the gradient is gradient descent methods. For example, if the loss function is ![equation](http://latex.codecogs.com/gif.latex?J(\theta)), then formula below is used for the gradient descent:

![equation](http://latex.codecogs.com/gif.latex?\theta_{j}=\theta_{j}-\alpha\frac{\partial}{\partial{\theta_j}}J(\theta))

In the formula, ![equation](http://latex.codecogs.com/gif.latex?\alpha) is the learning rate, indicating how long should we update the parameters through the negative gradient direction.

We have previously stated the OLS loss function in ![linear regression](https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/algorithm/1.linearRegression.md), following is the formula:

![equation](http://latex.codecogs.com/gif.latex?J(\theta)=\frac{1}{2}(X\theta-\overrightarrow{y})^T(X\theta-\overrightarrow{y}))



