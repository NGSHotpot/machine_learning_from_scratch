# Linear Regression

## Simple linear regression

Simple linear regression represent linear regression with only one independent variable. 

For example, we have independent variable ![equation](http://latex.codecogs.com/gif.latex?X=[x_1,x_2,...,x_n]) and dependent variable ![equation](http://latex.codecogs.com/gif.latex?Y=[y_1,y_2,...,y_n]). We are trying to find weights ![equation](http://latex.codecogs.com/gif.latex?w) and bias ![equation](http://latex.codecogs.com/gif.latex?b) so that we can model the value of the dependent variable using the following formula: ![equation](http://latex.codecogs.com/gif.latex?\hat{y_i}=wx_i+b) for i range from 1 to n.
Here ![equation](http://latex.codecogs.com/gif.latex?\hat{y_i}) is the predicted value, obviously, any ![equation](http://latex.codecogs.com/gif.latex?w) and ![equation](http://latex.codecogs.com/gif.latex?b) can be used to predict the dependent variable. What linear regression do is to find the best ![equation](http://latex.codecogs.com/gif.latex?w) and ![equation](http://latex.codecogs.com/gif.latex?b) that can minimize some predefined loss function.

## Loss function
A simple words for loss function, a loss function is defined to measure how bad the prediction is compared to the real values. For example, we have a 10 to predict, then 12 will be a better prediction than 15 (something like this). Many kinds of loss function could be used, here I will only introduce the Ordinary Least Squares (OLS) which is defined as: 

![equation](http://latex.codecogs.com/gif.latex?L=\sum_{i=1}^{n}(y_i-\hat{y_i})^2)

which also always used as:

![equation](http://latex.codecogs.com/gif.latex?L=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2)

I will introduce other loss function in multiple linear regression.

## Solution for simple linear regression

Math part, it is ok to skip this part and just look at the last to see the conclusions.

We clearly see that the loss function is larger or equal to zero,

![equation](http://latex.codecogs.com/gif.latex?L=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2\ge0)

and the loss function can tend to infinity. So to minimize the loss function, we can make the derivative of the loss function equal to zero. Then we do partial derivative for ![equation](http://latex.codecogs.com/gif.latex?w) and ![equation](http://latex.codecogs.com/gif.latex?b) respectively.

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{L}}{\partial{w}}=\frac{2}{n}\sum_{i=1}^{n}(y_i-wx_i-b)x_i=0)

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{L}}{\partial{b}}=\frac{2}{n}\sum_{i=1}^{n}(y_i-wx_i-b)=0)

Then we have:

(1): ![equation](http://latex.codecogs.com/gif.latex?w\sum_{i=1}^{n}x_i^2+b\sum_{i=1}^{n}x_i=\sum_{i=1}^{n}(x_iy_i))     

(2): ![equation](http://latex.codecogs.com/gif.latex?w\sum_{i=1}^{n}x_i+bn=\sum_{i=1}^{n}(y_i))     

Then let ![equation](http://latex.codecogs.com/gif.latex?(1)\times{n}-(2)\times{\sum_{i=1}^{n}x_i})

So then we have:

![equation](http://latex.codecogs.com/gif.latex?w=\frac{n\sum_{i=1}^{n}(x_iy_i)-(\sum_{i=1}^{n}x_i)(\sum_{i=1}^{n}y_i)}{n\sum_{i=1}^{n}x_i^2-(\sum_{i=1}^nx_i)^2}) 

with the value of ![equation](http://latex.codecogs.com/gif.latex?w), we can easily get:

![equation](http://latex.codecogs.com/gif.latex?b=\frac{\sum_{i=1}^{n}y_i-w\sum_{i=1}^{n}x_i}{n}) 

Finally with the two equations above, we solved the simple linear regression problem.


