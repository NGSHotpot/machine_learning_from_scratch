# Linear Regression

## Simple linear regression

Simple linear regression represent linear regression with only one independent variable. 

For example, we have independent variable ![equation](http://latex.codecogs.com/gif.latex?X=[x_1,x_2,...,x_n]) and dependent variable ![equation](http://latex.codecogs.com/gif.latex?Y=[y_1,y_2,...,y_n]). We are trying to find weights ![equation](http://latex.codecogs.com/gif.latex?w) and bias ![equation](http://latex.codecogs.com/gif.latex?b) so that we can model the value of the dependent variable using the following formula: ![equation](http://latex.codecogs.com/gif.latex?\hat{y_i}=wx_i+b) for i range from 1 to n.
Here ![equation](http://latex.codecogs.com/gif.latex?\hat{y_i}) is the predicted value, obviously, any ![equation](http://latex.codecogs.com/gif.latex?w) and ![equation](http://latex.codecogs.com/gif.latex?b) can be used to predict the dependent variable. What linear regression do is to find the best ![equation](http://latex.codecogs.com/gif.latex?w) and ![equation](http://latex.codecogs.com/gif.latex?b) that can minimize some predefined loss function.

### Loss function
A simple words for loss function, a loss function is defined to measure how bad the prediction is compared to the real values. For example, we have a 10 to predict, then 12 will be a better prediction than 15 (something like this). Many kinds of loss function could be used, here I will only introduce the Ordinary Least Squares (OLS) which is defined as: 

![equation](http://latex.codecogs.com/gif.latex?L=\sum_{i=1}^{n}(y_i-\hat{y_i})^2)

which also always used as:

![equation](http://latex.codecogs.com/gif.latex?L=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2)

I will introduce other loss function in multiple linear regression.

### Solution for simple linear regression

Math part, it is ok to skip this part and just look at the last to see the conclusions.

We clearly see that the loss function is larger or equal to zero,

![equation](http://latex.codecogs.com/gif.latex?L=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2\ge0)

and the loss function can tend to infinity. So to minimize the loss function, we can make the derivative of the loss function equal to zero. Then we do partial derivative for ![equation](http://latex.codecogs.com/gif.latex?w) and ![equation](http://latex.codecogs.com/gif.latex?b) respectively.

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{L}}{\partial{w}}=\frac{2}{n}\sum_{i=1}^{n}(y_i-wx_i-b)x_i=0)

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{L}}{\partial{b}}=\frac{2}{n}\sum_{i=1}^{n}(y_i-wx_i-b)=0)

Then we have:

(1): ![equation](http://latex.codecogs.com/gif.latex?w\sum_{i=1}^{n}x_i^2+b\sum_{i=1}^{n}x_i=\sum_{i=1}^{n}(x_iy_i))     

(2): ![equation](http://latex.codecogs.com/gif.latex?w\sum_{i=1}^{n}x_i+bn=\sum_{i=1}^{n}(y_i))     

Then let ![equation](http://latex.codecogs.com/gif.latex?(1)\times{n}-(2)\times{\sum_{i=1}^{n}x_i})

So then we have:

![equation](http://latex.codecogs.com/gif.latex?w=\frac{n\sum_{i=1}^{n}(x_iy_i)-(\sum_{i=1}^{n}x_i)(\sum_{i=1}^{n}y_i)}{n\sum_{i=1}^{n}x_i^2-(\sum_{i=1}^nx_i)^2}) 

with the value of ![equation](http://latex.codecogs.com/gif.latex?w), we can easily get:

![equation](http://latex.codecogs.com/gif.latex?b=\frac{\sum_{i=1}^{n}y_i-w\sum_{i=1}^{n}x_i}{n}) 

Finally with the two equations above, we solved the simple linear regression problem.

### Python solution from scratch

Belowing the the python codes solving the simple linear regression from scratch.

```python
import os, glob, math
import numpy as np

class linear_model:
  """
  linear model class, this is only for simple linear regression
  """
  def __init__(self):
    self.rmse = None
    pass
  
  def train(self, X, y):
    """
    @param X, independent variable, here requires the X to have only one variable.
    Example: X = np.array([1,2,3,4,...])
    @param y, dependent variable, example y = np.array([1,2,3,4,...])
    @return, this function will return weight paramether w and bias parameter b
    """
    n = len(X)
    if len(y) != n:
      print "Error, the input X, y should be the same length, while you have len(X)=%d and len(y)=%d"%(n, len(y))
    
    # following codes reformat the input, this function only for simple linear regression
    X = np.array(X)
    y = np.array(y)
    if len(X.shape) > 1:
      X = X.reshape(n)
    if len(y.shape) > 1:
      y = y.reshape(n)
    
    # get the parameter w and b
    sumx = np.sum(X)
    sumy = np.sum(y)
    xy = X.dot(y)
    xsquare = X.dot(X)
    
    w = (n*xy - sumx*sumy) / (n*xsquare - sumx**2)
    b = (sumy - w * sumx) / n
    self.w = w
    self.b = b
    return w, b
  
  def predict(self, X, y=None):
    """
    @param X, independent variable, here requires the X to have only one variable.
    Example: X = np.array([1,2,3,4,...])
    @param y, dependent variable, example y = np.array([1,2,3,4,...])
    @return, return the predicted dependent variables for input X, or if y is specified, also calculate the RMSE (root mean square error)
    """
    n = len(X)
    X = np.array(X)
    if len(X.shape) > 1:
      X = X.reshape(n)
    y_pred = self.w * X + self.b
    if y == None:
      self.y_pred = y_pred
    else:
      if n != len(y):
        print "Error, the input X, y should be the same length, while you have len(X)=%d and len(y)=%d"%(n, len(y))
      y = np.array(y)
      if len(y.shape) > 1:
        y = y.reshape(n)
      self.rmse = np.sqrt(np.sum(np.square(y-y_pred))) / n
    return y_pred
      
```

This is easy, and the code also can be find at ![simple linear regression](https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/codes/simple_linear_regression.py)

## Multiple linear regression

After we have get the solution for simple linear regression, we will come to check the multiple linear regression. The difference is that, the independent variable could now have multiple variables, for ![equation](http://latex.codecogs.com/gif.latex?X=[x_1,x_2,...,x_n]), each sample could be described by many parameters, ![equation](http://latex.codecogs.com/gif.latex?x_i=[x_{i1},x_{i2},...,x_{im}]), this is each sample have m variables.

Then for each dependent variable, we could model is with the independent variables as 

![equation](http://latex.codecogs.com/gif.latex?\hat{y_i}=\sum_{j=1}^{m}w_jx_{ij}+b=Wx_i^T+b)

where 

![equation](http://latex.codecogs.com/gif.latex?W=[w_1,w_2,...,w_m])

The simple linear regression is a specific case of multiple linear regression, so the following part of solution can also be used to do simple linear regression. And we use the Ordinary Least Squares (OLS): 

![equation](http://latex.codecogs.com/gif.latex?L=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2)

Similar as in the simple linear regression part, we could make the patial derivatives of the loss function to be zero. Here is how we process:

![equation](http://latex.codecogs.com/gif.latex?L=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2=\frac{1}{n}\sum_{i=1}^{n}(y_i-Wx_i^T-b)^2)

Then we rewrite the parameters and independent variables of each sample, for example,

![equation](http://latex.codecogs.com/gif.latex?\theta=[w_1,w_2,...,w_m,b])

and

![equation](http://latex.codecogs.com/gif.latex?x_i=[x_{i1},x_{i2},...,x_{im},1])

both ![equation](http://latex.codecogs.com/gif.latex?\theta) and ![equation](http://latex.codecogs.com/gif.latex?x_i) are vectors with size ![equation](http://latex.codecogs.com/gif.latex?1\times{(m+1)}).Then we can get the linear model with:

![equation](http://latex.codecogs.com/gif.latex?\hat{y_i}=\sum_{j=1}^{m}w_jx_{ij}+b=\theta{x_i}^T)

Then we re-formula the loss function as

![equation](http://latex.codecogs.com/gif.latex?L=\frac{1}{n}\sum_{i=1}^{n}(y_i-Wx_i^T-b)^2=\frac{1}{n}(\theta{x_i}^T-y_i)^2=\frac{1}{n}(\theta{X}^T-y_i)^T(\theta{X}^T-y_i))

This is similar to the following loss function (only divided a constant number):

![equation](http://latex.codecogs.com/gif.latex?J(\theta)=\frac{1}{2}(\theta{X}^T-y_i)^T(\theta{X}^T-y_i))

Then we calculate the derivatives of the loss function to every parameter as a parameter vector, which is also called gradient. The gradient of the loss function is calculated as following:

![equation](http://latex.codecogs.com/gif.latex?\nabla_{\theta}J(\theta))
