# Linear Regression

## Simple linear regression

Simple linear regression represent linear regression with only one independent variable. 

For example, we have independent variable ![equation](http://latex.codecogs.com/gif.latex?X=[x_1,x_2,...,x_n]) and dependent variable ![equation](http://latex.codecogs.com/gif.latex?Y=[y_1,y_2,...,y_n]). We are trying to find weights ![equation](http://latex.codecogs.com/gif.latex?w) and bias ![equation](http://latex.codecogs.com/gif.latex?b) so that we can model the value of the dependent variable using the following formula: ![equation](http://latex.codecogs.com/gif.latex?\hat{y_i}=wx_i+b) for i range from 1 to n.
Here ![equation](http://latex.codecogs.com/gif.latex?\hat{y_i}) is the predicted value, obviously, any ![equation](http://latex.codecogs.com/gif.latex?w) and ![equation](http://latex.codecogs.com/gif.latex?b) can be used to predict the dependent variable. What linear regression do is to find the best ![equation](http://latex.codecogs.com/gif.latex?w) and ![equation](http://latex.codecogs.com/gif.latex?b) that can minimize some predefined loss function.

## Loss function
A simple words for loss function, a loss function is defined to measure how bad the prediction is compared to the real values. For example, we have a 10 to predict, then 12 will be a better prediction than 15 (something like this). Many kinds of loss function could be used, here I will only introduce the Ordinary Least Squares (OLS) which is defined as: 

![equation](http://latex.codecogs.com/gif.latex?L=\sum_{i=1}^{n}(y_i-\hat{y_i})^2)

which also always used as:

![equation](http://latex.codecogs.com/gif.latex?L=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2)

I will introduce other loss function in multiple linear regression.

## Solution for simple linear regression

We clearly see that the loss function is larger or equal to zero,

![equation](http://latex.codecogs.com/gif.latex?L=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2\ge0)

and the loss function can tend to infinity. So to minimize the loss function, we can make the derivative of the loss function equal to zero. 
