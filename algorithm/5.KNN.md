# K Nearest Neighbor (KNN)

## Introduction

Previously, we have introduced that the logistic regression can be used to classification problems, but only binary classification problems.
Logistic can only decide an input sample belong to whether to A-class or B-class, it can not used to classify sample into multiple classes,
A-class, B-class, C-class, etc, for example.

The most simple and direct algorithm for multi-class classification problem is the k nearest neighbor (KNN) algorithm which is based on the
proverb: "Birds of a feather flock together". 

## KNN algorithm

In KNN training process, the model store the training data together with corresponding labels (categories the data belong to), which is
quite simple.

In KNN testing process, for each input test sample, the algorithm find k nearest samples from the stored training data. Count the categories
of the k training samples. The most frequently appeared category is then assigned as the predicting category of the input test sample.

## Distance

When measuring the k nearest neighbors from the test sample, a specific distance should be defined in advance. Most frequently used distance
is the Euclidean distance. For example, if we have two points ![equation](http://latex.codecogs.com/gif.latex?X_1=(a_1,a_2,...,a_m)) 
and ![equation](http://latex.codecogs.com/gif.latex?X_2=(b_1,b_2,...,b_m)), then the Euclidean distance between the two points are defined as:

![equation](http://latex.codecogs.com/gif.latex?d(X_1,X_2)=\sqrt{\sum_{i=1}^{m}(a_i-b_i)^2})

Euclidean distance reach its minimum 0 when the two points are in the same position, and Euclidean distance has max infinity. The larger the
Euclidean distance is, the more far away for the two points.

Also we have other famous distances definations such as:

1. Pearson Correlation Coefficient (PCC)

![equation](http://latex.codecogs.com/gif.latex?PCC(X_1,X_2)=\frac{\sum_{i=1}^{m}(a_i-\bar{a})(b_i-\bar{b})}{\sqrt{\sum_{i=1}^{m}(a_i-\bar{a})^2}\sqrt{\sum_{i=1}^{m}(b_i-\bar{b})^2}})

Where 

![equation](http://latex.codecogs.com/gif.latex?\bar{a}=\frac{1}{m}\sum_{i=1}^{m}a_i)

![equation](http://latex.codecogs.com/gif.latex?\bar{b}=\frac{1}{m}\sum_{i=1}^{m}b_i)

2. Cosine distance

![equation](http://latex.codecogs.com/gif.latex?cosine(X_1,X_2)=\frac{\sum_{i=1}^{m}a_ib_i}{\sqrt{\sum_{i=1}^{m}a_i^2}\sqrt{\sum_{i=1}^{m}b_i^2}})

For both PCC and cosine distance, the larger the value is, the closer the two points are. Also there are too many other distance or similarity measurement methods.

## Python implementation of KNN from scratch

Codes is very simple as following, for the codes below, the test time will be long is the training data is large. And this can be improved using kd-tree data structure (looking forward, I will add later).

```python
import numpy as np
from scipy.stats import mode

class KNearestNeighbor:
  """
  Implementation the most straight forward KNN algorithms
  """
  def __init__(self):
    self.trainX = None
    self.trainY = None
    
  def train(self, X, y):
    """
    Just store the training data
    @param X, n times m size with n samples and each sample have m paramters, for example: X = np.array([[1,2],[3,4],[2,3],...])
    @param y, list with size n, for example: y = np.array([1,2,3,4,...])
    """
    self.trainX = X
    self.trainY = y
    self.predY = None
    
  def test(self, X, K=3, dist_type="euclidean"):
    """
    @param X, n times m size with n samples and each sample have m paramters, for example: X = np.array([[1,2],[3,4],[2,3],...])
    @param K, hyper parameter for KNN algorithm
    @param dist_type, the type to calculate distance
    @return, Ypred, the predicted categories for the n samples
    """
    distance_types = set(["euclidean", "abs"])
    if dist_type not in distance_types:
      print "Error, the distance types should only be", distance_types
      return
    X = np.array(X)
    num_test = X.shape[0]
    Ypred = np.zeros(num_test, dtype = self.trainY.dtype)
    
    for i in xrange(num_test):
      if dist_type == "euclidean":
        distances = np.sum(np.abs(self.trainX - X[i, :]), axis = 1)
      else:
        # did not sqrt because it's the same
        distances = np.sum(np.square(self.trainX - X[i, :]), axis = 1)
      sort_index = np.argsort(distances)
      min_index = sort_index[:K]
      k_ytr = [self.trainY[tmp_index] for tmp_index in min_index]
      pred, pred_count = mode(k_ytr)
      if pred_count[0] == 1:
        Ypred[i] = -1
      else:
        Ypred[i] = pred[0]
    self.predY = Ypred
    return Ypred
  

```

Here I only implement Euclidean distance and absolute distance for above KNN codes, see also at [KNN]()

## Summary

KNN can be used for multi-class classification problems, which is straight forward, and easy to understand and implement. Then I will introduce how softmax function, an extension of logistic regression can be used to solve multi-class classification problems.



